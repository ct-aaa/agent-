import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import os


# === 1. å®šä¹‰æ”¹è¿›åçš„ CNN (BetterCNN) ===
class BetterCNN(nn.Module):
    def __init__(self):
        super(BetterCNN, self).__init__()

        # ç¬¬ä¸€å±‚å·ç§¯å—: Conv -> BatchNorm -> ReLU -> MaxPool
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)  # æ–°å¢: å½’ä¸€åŒ–ï¼Œæå‡æŠ—å¹²æ‰°èƒ½åŠ›

        # ç¬¬äºŒå±‚å·ç§¯å—
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)  # æ–°å¢

        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()

        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(64 * 7 * 7, 128)
        self.dropout = nn.Dropout(0.5)  # æ–°å¢: ä¸¢å¼ƒ50%ç¥ç»å…ƒï¼Œé˜²æ­¢æ­»è®°ç¡¬èƒŒ
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        # Block 1
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.pool(x)

        # Block 2
        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)
        x = self.pool(x)

        # Flatten
        x = x.view(-1, 64 * 7 * 7)

        # FC Block
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)  # åº”ç”¨ Dropout
        x = self.fc2(x)
        return x


# === 2. è®­ç»ƒé…ç½® ===
def train_optimized():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”¥ ä½¿ç”¨è®¾å¤‡: {device}")

    # --- ç­–ç•¥A: æ•°æ®å¢å¼º (Data Augmentation) ---
    train_transform = transforms.Compose([
        transforms.RandomRotation(15),  # éšæœºæ—‹è½¬ -15~15åº¦ (æ¨¡æ‹Ÿå†™æ­ªäº†)
        transforms.RandomAffine(  # éšæœºä»¿å°„å˜æ¢
            degrees=0,
            translate=(0.1, 0.1),  # ä¸Šä¸‹å·¦å³å¹³ç§» 10% (æ¨¡æ‹Ÿæ²¡å†™åœ¨æ­£ä¸­é—´)
            scale=(0.9, 1.1)  # å¤§å°ç¼©æ”¾ 0.9~1.1å€
        ),
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    # æµ‹è¯•é›†ä¸éœ€è¦å¢å¼ºï¼Œåªéœ€è¦å½’ä¸€åŒ–
    test_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])

    # åŠ è½½æ•°æ®
    train_dataset = datasets.MNIST('./data_cache', train=True, download=True, transform=train_transform)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)

    # åˆå§‹åŒ–æ¨¡å‹
    model = BetterCNN().to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()

    # --- ç­–ç•¥B: å¢åŠ è®­ç»ƒè½®æ•° (Epochs) ---
    epochs = 5
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ (è®¡åˆ’ {epochs} è½®)...")

    model.train()
    for epoch in range(1, epochs + 1):
        total_loss = 0
        correct = 0
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

        # æ‰“å°æ¯ä¸€è½®çš„æˆç»©
        acc = 100. * correct / len(train_loader.dataset)
        print(f"Epoch {epoch}/{epochs} | å¹³å‡Loss: {total_loss / len(train_loader):.4f} | è®­ç»ƒé›†å‡†ç¡®ç‡: {acc:.2f}%")

    # ä¿å­˜æ¨¡å‹
    save_path = "../models/model_a.pth"
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists("../models"):
        os.makedirs("../models")

    torch.save(model.state_dict(), save_path)
    print(f"âœ… ä¼˜åŒ–åçš„æ¨¡å‹å·²ä¿å­˜: {save_path}")


if __name__ == '__main__':
    train_optimized()