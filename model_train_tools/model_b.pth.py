import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import os

# === 1. é…ç½® ===
DATASET_DIR = "../datasets/dataset_B"
MODEL_SAVE_PATH = "../models/model_b.pth"
CLASS_SAVE_PATH = "../models/model_b_classes.txt"
BATCH_SIZE = 32
EPOCHS = 10  # ç¨å¾®å¤šè®­ç»ƒå‡ è½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# === 2. è‡ªå®šä¹‰æ•°æ®é›†è¯»å–å™¨ (è¯»å– label.txt) ===
class CustomDataset(Dataset):
    def __init__(self, root_dir, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        self.image_paths = []
        self.labels = []
        self.classes = []

        # è¯»å– label.txt
        label_file = os.path.join(root_dir, "label.txt")
        if not os.path.exists(label_file):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ ‡ç­¾æ–‡ä»¶: {label_file}")

        # ç¬¬ä¸€éï¼šæ”¶é›†æ‰€æœ‰ç±»åˆ«åç§°å¹¶æ’åºï¼Œå»ºç«‹ç´¢å¼•
        raw_labels = []
        with open(label_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for line in lines:
                parts = line.strip().split()
                if len(parts) >= 2:
                    raw_labels.append(parts[1].lower())

        self.classes = sorted(list(set(raw_labels)))  # ['bird', 'car', 'cat'...]
        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}

        print(f"æ£€æµ‹åˆ° {len(self.classes)} ä¸ªç±»åˆ«: {self.classes}")

        # ä¿å­˜ç±»åˆ«åˆ°æ–‡ä»¶ï¼Œä¾› tools.py ä½¿ç”¨
        if not os.path.exists("models"): os.makedirs("models")
        with open(CLASS_SAVE_PATH, 'w', encoding='utf-8') as f:
            for cls in self.classes:
                f.write(f"{cls}\n")

        # ç¬¬äºŒéï¼šåŠ è½½æ•°æ®è·¯å¾„å’Œæ ‡ç­¾ç´¢å¼•
        with open(label_file, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) >= 2:
                    filename = parts[0] + ".png"  # å‡è®¾å›¾ç‰‡æ˜¯ pngï¼Œæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                    # å¦‚æœæ–‡ä»¶åé‡Œå·²ç»å¸¦äº†åç¼€ï¼Œå°±ä¸è¦åŠ  .png
                    if os.path.exists(os.path.join(root_dir, parts[0])):
                        filename = parts[0]

                    img_path = os.path.join(root_dir, filename)
                    label_name = parts[1].lower()

                    if os.path.exists(img_path):
                        self.image_paths.append(img_path)
                        self.labels.append(self.class_to_idx[label_name])

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert("RGB")
        label = self.labels[idx]

        if self.transform:
            image = self.transform(image)

        return image, label


# === 3. è®­ç»ƒæµç¨‹ ===
def train():
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ Dataset Bï¼Œä½¿ç”¨è®¾å¤‡: {DEVICE}")

    # æ•°æ®å¢å¼º
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # ResNet æ ‡å‡†è¾“å…¥
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    dataset = CustomDataset(DATASET_DIR, transform=transform)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    # åŠ è½½é¢„è®­ç»ƒ ResNet18
    model = models.resnet18(weights='DEFAULT')

    # === å…³é”®æ­¥éª¤ï¼šä¿®æ”¹å…¨è¿æ¥å±‚ ===
    # åŸå§‹ ResNet18 è¾“å‡º 1000 ç±»ï¼Œæˆ‘ä»¬è¦æ”¹ä¸º dataset çš„ç±»åˆ«æ•° (é€šå¸¸æ˜¯10)
    num_ftrs = model.fc.in_features
    model.fc = nn.Linear(num_ftrs, len(dataset.classes))

    model = model.to(DEVICE)

    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
    criterion = nn.CrossEntropyLoss()

    model.train()
    for epoch in range(EPOCHS):
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in dataloader:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

        acc = 100 * correct / total
        print(f"Epoch {epoch + 1}/{EPOCHS} | Loss: {running_loss / len(dataloader):.4f} | Acc: {acc:.2f}%")

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), MODEL_SAVE_PATH)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜è‡³: {MODEL_SAVE_PATH}")
    print(f"âœ… ç±»åˆ«æ–‡ä»¶å·²ä¿å­˜è‡³: {CLASS_SAVE_PATH}")


if __name__ == "__main__":
    train()