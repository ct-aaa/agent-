import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader, random_split
import os
import time

# --- é…ç½®åŒºåŸŸ ---
DATASET_PATH = 'datasets/dataset_C'
# ğŸ’¡ è¯»å–ä½ åˆšæ‰é‚£ä¸ªå·²ç»æœ‰ 60% å‡†ç¡®ç‡çš„æ¨¡å‹ä½œä¸ºèµ·ç‚¹
LOAD_MODEL_PATH = 'models/model_c_finetuned.pth'
SAVE_MODEL_PATH = 'models/model_c_pro.pth'
SAVE_TXT_PATH = 'models/model_c_classes.txt'
BATCH_SIZE = 32
EPOCHS = 30
# å­¦ä¹ ç‡å†ä½ä¸€ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯åœ¨å¾®è°ƒä¸€ä¸ªå·²ç»è¿˜ä¸é”™çš„æ¨¡å‹
LEARNING_RATE = 5e-5


def fine_tune_pro():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ å¯åŠ¨ç»ˆæä¼˜åŒ–æ¨¡å¼ (Device: {device})")

    # ==================================================
    # 1. å¼ºåŠ›æ•°æ®å¢å¼º (å…³é”®æ”¹è¿›ç‚¹ï¼)
    # ==================================================
    train_transform = transforms.Compose([
        # RandomResizedCrop æ˜¯æ ¸å¿ƒï¼šå®ƒä¼šéšæœºæˆªå–å›¾ç‰‡çš„ä¸€éƒ¨åˆ†å¹¶æ”¾å¤§
        # è¿™è¿«ä½¿æ¨¡å‹å­¦ä¹ å±€éƒ¨ç‰¹å¾ï¼Œè€Œä¸æ˜¯æ­»è®°ç¡¬èƒŒæ•´å¼ å›¾
        transforms.RandomResizedCrop(224, scale=(0.6, 1.0)),

        transforms.RandomHorizontalFlip(),
        transforms.RandomRotation(20),  # å¢åŠ æ—‹è½¬è§’åº¦
        transforms.ColorJitter(brightness=0.3, contrast=0.3),  # å¢åŠ é¢œè‰²å¹²æ‰°
        transforms.ToTensor(),

        # éšæœºæ“¦é™¤ï¼šéšæœºæŠŠå›¾ç‰‡æŒ–æ‰ä¸€å—ï¼Œå¼ºè¿«æ¨¡å‹é å‰©ä½™éƒ¨åˆ†è¯†åˆ«
        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15)),

        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    # åŠ è½½æ•°æ®
    full_dataset = datasets.ImageFolder(root=DATASET_PATH)
    classes = full_dataset.classes
    num_classes = len(classes)

    train_size = int(0.8 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_subset, val_subset = random_split(full_dataset, [train_size, val_size])

    class TransformedDataset(torch.utils.data.Dataset):
        def __init__(self, subset, transform=None):
            self.subset = subset
            self.transform = transform

        def __getitem__(self, index):
            x, y = self.subset[index]
            if self.transform: x = self.transform(x)
            return x, y

        def __len__(self): return len(self.subset)

    train_loader = DataLoader(TransformedDataset(train_subset, train_transform), batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(TransformedDataset(val_subset, val_transform), batch_size=BATCH_SIZE, shuffle=False)

    # ==================================================
    # 2. æ¨¡å‹ç»“æ„é‡å»º (ä¿æŒä¸€è‡´)
    # ==================================================
    print(f"ğŸ—ï¸  åŠ è½½ç°æœ‰æƒé‡: {LOAD_MODEL_PATH}")
    model = models.mobilenet_v3_small(weights=None)

    # é‡å»ºåˆ†ç±»å¤´ (æ³¨æ„ï¼šè¿™é‡Œæˆ‘æŠŠ Dropout åŠ å¤§åˆ°äº† 0.5)
    num_ftrs = model.classifier[3].in_features
    model.classifier[3] = nn.Sequential(
        nn.Dropout(p=0.5),  # ğŸ’¡ åŠ å¤§ Dropoutï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        nn.Linear(num_ftrs, num_classes)
    )

    # åŠ è½½æƒé‡
    if os.path.exists(LOAD_MODEL_PATH):
        state_dict = torch.load(LOAD_MODEL_PATH, map_location=device)
        model.load_state_dict(state_dict)
    else:
        print("âŒ æ‰¾ä¸åˆ°ä¸Šä¸€è½®çš„æ¨¡å‹æ–‡ä»¶ï¼")
        return

    # å…¨ç½‘è§£å†»
    for param in model.parameters():
        param.requires_grad = True

    model = model.to(device)

    # ==================================================
    # 3. æŸå¤±å‡½æ•°æ”¹è¿› & ä¼˜åŒ–å™¨
    # ==================================================
    # ğŸ’¡ Label Smoothing: æ ‡ç­¾å¹³æ»‘ï¼Œé˜²æ­¢æ¨¡å‹ç›²ç›®è‡ªä¿¡
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    # ğŸ’¡ Weight Decay: åŠ å¤§åˆ° 0.005ï¼Œå¼ºåŠ›æŠ‘åˆ¶è¿‡æ‹Ÿåˆ
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.005)

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=3, verbose=True)

    # ==================================================
    # 4. è®­ç»ƒå¾ªç¯
    # ==================================================
    best_acc = 0.0

    # é‡æ–°è·‘ä¸€ééªŒè¯é›†ï¼Œçœ‹çœ‹èµ·ç‚¹åœ¨å“ªé‡Œ
    model.eval()
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (preds == labels).sum().item()

    start_acc = val_correct / val_total
    best_acc = start_acc
    print(f"ğŸ å½“å‰åŸºå‡†å‡†ç¡®ç‡: {start_acc:.1%}")

    for epoch in range(EPOCHS):
        start_time = time.time()

        # Train
        model.train()
        running_loss = 0.0
        train_correct = 0
        train_total = 0

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (preds == labels).sum().item()

        epoch_loss = running_loss / train_size
        train_acc = train_correct / train_total

        # Validate
        model.eval()
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (preds == labels).sum().item()

        val_acc = val_correct / val_total
        elapsed = time.time() - start_time

        # åªæœ‰å½“éªŒè¯é›†å‡†ç¡®ç‡æ²¡æ‰å¤ªå¤šçš„æ—¶å€™ï¼Œæ‰æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_acc)

        print(
            f"Epoch {epoch + 1}/{EPOCHS} [{elapsed:.0f}s] Loss: {epoch_loss:.4f} | Train: {train_acc:.1%} | Val: {val_acc:.1%}",
            end="")

        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), SAVE_MODEL_PATH)
            with open(SAVE_TXT_PATH, 'w', encoding='utf-8') as f:
                f.write('\n'.join(classes))
            print(" â­ New Best!")
        else:
            print("")

    print(f"\nâœ… ä¼˜åŒ–å®Œæˆã€‚æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.1%}")
    print(f"æ¨¡å‹å·²ä¿å­˜: {SAVE_MODEL_PATH}")


if __name__ == "__main__":
    fine_tune_pro()